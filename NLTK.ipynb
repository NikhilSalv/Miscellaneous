{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we are in a situation to create Sentimental Analysis model, we have the dataset available. But the problem here is, the machine does not understand the sentences of any language. We have to clean those datasets by using stopwords, deleting punctuations and deletoing many more relevant things inside data and we have to make it upto that level where we can feed this data to machine learning and deep learning algorithms, so that we get the desired output with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import necessary libraries\n",
    "import nltk\n",
    "import string\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLTK Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\NIKIL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\NIKIL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Data', 'new', 'oil', ',', 'A.I', '.', 'last', 'invention']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def rem_stopwords(text):\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    words_tokens = word_tokenize(text)\n",
    "    filtered_text = [word for word in words_tokens if word not in stop_words]\n",
    "    return filtered_text\n",
    "\n",
    "ex_text = \"Data is new oil, A.I. is the last invention\"\n",
    "rem_stopwords(ex_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming\n",
    "Using Stemming, we will process of getting the root form of a word. Root or Stem is the part to which inflextional affixes (like -ed, -ize , etc. ) are added. We would create the stem words by removing the prefix or the suffix of the words. Hence, stemming of the words may not result in the actual words. \n",
    "e.g.:   Mangoes >> Mango\n",
    "        Boys >> Boy\n",
    "\n",
    "If our sentences are not in tokens, then we need to convert it into tokens. After we convert strings of texts into tokens, then we can convert those word tokens into their root form. These are the porter stemmer, the snowball stemmer, and the Lancaster stemmer. We usually use ported stemmer among them.\n",
    "\n",
    "##### Whenever grammar is conidered as important as a part of the operation, we should not use stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['use',\n",
       " 'stem',\n",
       " ',',\n",
       " 'we',\n",
       " 'will',\n",
       " 'process',\n",
       " 'of',\n",
       " 'get',\n",
       " 'the',\n",
       " 'root',\n",
       " 'form',\n",
       " 'of',\n",
       " 'a',\n",
       " 'word',\n",
       " '.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import nltk's porter stemmer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "stem1 = PorterStemmer()\n",
    "\n",
    "# stem words in the list of tokenized words\n",
    "\n",
    "def s_words(text):\n",
    "    word_tokens = word_tokenize(text)\n",
    "    stems = [stem1.stem(word) for word in word_tokens]\n",
    "    return stems\n",
    "\n",
    "text = \"Using Stemming, we will process of getting the root form of a word.\"\n",
    "s_words(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization\n",
    "Stemming and lemmatization do the same but the only difference is that lemmatization ensures that root word belongs to the language. With the help of lemmatization we will get the valid words. In NLTK, we use WordLemmatizer to get the lemmas of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\NIKIL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Data be the new revolution in the world , in a day one individual would generate terabytes of data'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "lemma = wordnet.WordNetLemmatizer()\n",
    "nltk.download('wordnet')\n",
    "# Lemmatize string\n",
    "def lemmatize_word(text):\n",
    "    word_tokens = word_tokenize(text)\n",
    "    # provide some context which has parts of speech.\n",
    "#     lemmas = [lemma.lemmatize(word,pos = 'v') for word in word_tokens]\n",
    "    return (' '.join([lemma.lemmatize(word,pos = 'v') for word in word_tokens]))\n",
    "\n",
    "text = 'Data is the new revolution in the world, in a day one individual would generate terabytes of data'\n",
    "lemmatize_word(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Lemmatization with appropriate POS tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\NIKIL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Data', 'NNP'),\n",
       " ('is', 'VBZ'),\n",
       " ('the', 'DT'),\n",
       " ('new', 'JJ'),\n",
       " ('revolution', 'NN'),\n",
       " ('in', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('world', 'NN'),\n",
       " (',', ','),\n",
       " ('in', 'IN'),\n",
       " ('a', 'DT'),\n",
       " ('day', 'NN'),\n",
       " ('one', 'CD'),\n",
       " ('individual', 'NN'),\n",
       " ('would', 'MD'),\n",
       " ('generate', 'VB'),\n",
       " ('terabytes', 'NNS'),\n",
       " ('of', 'IN'),\n",
       " ('data', 'NNS')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "sentence = \"Data is the new revolution in the world, in a day one individual would generate terabytes of data\"\n",
    "def pos_tagging(text):\n",
    "    word_tokenised = nltk.word_tokenize(text)\n",
    "    return (nltk.pos_tag(word_tokenised))\n",
    "\n",
    "pos_tagging(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chunking\n",
    "Process of extracting phrases from unstructured text and give them more structure to it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP the/DT little/JJ red/JJ parrot/NN)\n",
      "  is/VBZ\n",
      "  flying/VBG\n",
      "  in/IN\n",
      "  (NP the/DT sky/NN))\n",
      "(NP the/DT little/JJ red/JJ parrot/NN)\n",
      "(NP the/DT sky/NN)\n"
     ]
    }
   ],
   "source": [
    "from nltk import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "def chunking(text,grammar):\n",
    "    word_tokens = word_tokenize(text)\n",
    "    word_pos = pos_tag(word_tokens)\n",
    "    chunkParser = nltk.RegexpParser(grammar)\n",
    "    tree = chunkParser.parse(word_pos)\n",
    "    for subtree in tree.subtrees():\n",
    "        print(subtree)\n",
    "        \n",
    "sentence = 'the little red parrot is flying in the sky'\n",
    "grammar = \"NP:{<DT>?<JJ>*<NN>}\"\n",
    "chunking(sentence,grammar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove numbers!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My  favorite numbers are  and \n"
     ]
    }
   ],
   "source": [
    "s = \"My 2 favorite numbers are 7 and 10\"\n",
    "lst = re.sub('[0-7]',\"\",s)\n",
    "print(lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'weateher is cloudy. possibility if rain is high today!'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def lowercase(text):\n",
    "    return text.lower()\n",
    "inpur_str = \"Weateher is Cloudy. Possibility if Rain is High Today!\"\n",
    "lowercase(inpur_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "current_path = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
